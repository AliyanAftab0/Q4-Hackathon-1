"use strict";(globalThis.webpackChunke_book=globalThis.webpackChunke_book||[]).push([[5918],{4378:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>c,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"vla-agents","title":"VLA Agents","description":"The convergence of LLMs and Robotics.","source":"@site/docs/06-vla-agents.md","sourceDirName":".","slug":"/vla-agents","permalink":"/Q4-Hackathon-1/docs/vla-agents","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"VLA Agents"}}');var s=t(4848),a=t(8453);const r={title:"VLA Agents"},c="Vision-Language-Action (VLA) Agents",i={},l=[{value:"RT-2 and PaLM-E",id:"rt-2-and-palm-e",level:2},{value:"Architecture",id:"architecture",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"vision-language-action-vla-agents",children:"Vision-Language-Action (VLA) Agents"})}),"\n",(0,s.jsx)(n.p,{children:"The convergence of LLMs and Robotics."}),"\n",(0,s.jsx)(n.h2,{id:"rt-2-and-palm-e",children:"RT-2 and PaLM-E"}),"\n",(0,s.jsx)(n.p,{children:"Google's RT-2 (Robotic Transformer 2) demonstrates how VLA models can generalize to new instructions."}),"\n",(0,s.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Input: Text + Image\r\nOutput: Robot Actions (Tokenized)"})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>c});var o=t(6540);const s={},a=o.createContext(s);function r(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);