"use strict";(globalThis.webpackChunke_book=globalThis.webpackChunke_book||[]).push([[4540],{4997:(n,o,e)=>{e.r(o),e.d(o,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"part-5-control/biped-locomotion","title":"17. Biped Locomotion","description":"Walking is \\"controlled falling\\".","source":"@site/docs/part-5-control/biped-locomotion.md","sourceDirName":"part-5-control","slug":"/part-5-control/biped-locomotion","permalink":"/Q4-Hackathon-1/docs/part-5-control/biped-locomotion","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"16. Dynamics & Control","permalink":"/Q4-Hackathon-1/docs/part-5-control/dynamics-and-control"},"next":{"title":"18. Computer Vision for Robots","permalink":"/Q4-Hackathon-1/docs/part-6-perception/computer-vision-for-robots"}}');var r=e(4848),i=e(8453);const s={},l="17. Biped Locomotion",c={},a=[{value:"The Inverted Pendulum Model",id:"the-inverted-pendulum-model",level:2},{value:"Reinforcement Learning (RL) for Locomotion",id:"reinforcement-learning-rl-for-locomotion",level:2}];function d(n){const o={blockquote:"blockquote",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(o.header,{children:(0,r.jsx)(o.h1,{id:"17-biped-locomotion",children:"17. Biped Locomotion"})}),"\n",(0,r.jsx)(o.p,{children:'Walking is "controlled falling".'}),"\n",(0,r.jsx)(o.h2,{id:"the-inverted-pendulum-model",children:"The Inverted Pendulum Model"}),"\n",(0,r.jsxs)(o.p,{children:["We approximate the robot as an ",(0,r.jsx)(o.strong,{children:"Linear Inverted Pendulum (LIP)"}),"."]}),"\n",(0,r.jsxs)(o.ul,{children:["\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.strong,{children:"CoM"})," moves at constant height."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.strong,{children:"ZMP (Zero Moment Point)"})," tracks the center of pressure."]}),"\n"]}),"\n",(0,r.jsx)(o.h2,{id:"reinforcement-learning-rl-for-locomotion",children:"Reinforcement Learning (RL) for Locomotion"}),"\n",(0,r.jsxs)(o.p,{children:["Classical control is robust but hard to tune. ",(0,r.jsx)(o.strong,{children:"Deep RL"})," (PPO) is the new standard."]}),"\n",(0,r.jsxs)(o.ul,{children:["\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.strong,{children:"Sim-to-Real"}),": Train in Isaac Sim with randomized terrain."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.strong,{children:"Observation"}),": Joint positions, velocities, IMU."]}),"\n",(0,r.jsxs)(o.li,{children:[(0,r.jsx)(o.strong,{children:"Action"}),": Target joint angles (PD setpoints)."]}),"\n"]}),"\n",(0,r.jsxs)(o.blockquote,{children:["\n",(0,r.jsx)(o.p,{children:'"The robot learns to walk by trying millions of times in simulation."'}),"\n"]})]})}function h(n={}){const{wrapper:o}={...(0,i.R)(),...n.components};return o?(0,r.jsx)(o,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,o,e)=>{e.d(o,{R:()=>s,x:()=>l});var t=e(6540);const r={},i=t.createContext(r);function s(n){const o=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(o):{...o,...n}},[o,n])}function l(n){let o;return o=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:s(n.components),t.createElement(i.Provider,{value:o},n.children)}}}]);